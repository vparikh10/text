{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import and setup modules we'll be using in this notebook\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore\n",
    "\n",
    "def head(stream, n=10):\n",
    "    \"\"\"Convenience fnc: return the first `n` elements of the stream, as plain list.\"\"\"\n",
    "    return list(itertools.islice(stream, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import smart_open, simple_preprocess\n",
    "from gensim.corpora.wikicorpus import _extract_pages, filter_wiki\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "\n",
    "def iter_wiki(dump_file):\n",
    "    \"\"\"Yield each article from the Wikipedia dump, as a `(title, tokens)` 2-tuple.\"\"\"\n",
    "    ignore_namespaces = 'Wikipedia Category File Portal Template MediaWiki User Help Book Draft'.split()\n",
    "    for title, text, pageid in _extract_pages(smart_open(dump_file)):\n",
    "        text = filter_wiki(text)\n",
    "        tokens = tokenize(text)\n",
    "        if len(tokens) < 50 or any(title.startswith(ns + ':') for ns in ignore_namespaces):\n",
    "            continue  # ignore short articles and various meta-articles\n",
    "        yield title, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April [u'april', u'fourth', u'month', u'year', u'comes', u'march', u'days', u'april', u'begins', u'day']\n",
      "August [u'august', u'eighth', u'month', u'year', u'gregorian', u'calendar', u'coming', u'july', u'september', u'days']\n",
      "Art [u'painting', u'renoir', u'work', u'art', u'art', u'activity', u'creation', u'people', u'importance', u'attraction']\n",
      "A [u'page', u'letter', u'alphabet', u'indefinite', u'article', u'article', u'grammar', u'uses', u'disambiguation', u'letter']\n",
      "Air [u'air', u'fan', u'air', u'air', u'earth', u'atmosphere', u'clear', u'gas', u'living', u'things']\n",
      "Autonomous communities of Spain [u'spain', u'divided', u'parts', u'called', u'autonomous', u'communities', u'autonomous', u'means', u'autonomous', u'communities']\n",
      "Alan Turing [u'statue', u'alan', u'turing', u'rebuild', u'machine', u'alan', u'turing', u'alan', u'mathison', u'turing']\n",
      "Alanis Morissette [u'alanis', u'nadine', u'morissette', u'born', u'june', u'grammy', u'award', u'winning', u'canadian', u'american']\n"
     ]
    }
   ],
   "source": [
    "# only use simplewiki in this tutorial (fewer documents)\n",
    "# the full wiki dump is exactly the same format, but larger\n",
    "\n",
    "# r'/Users/viral.parikh/Desktop/External_Datasets/kaggle/bbc/bbc_small'\n",
    "\n",
    "stream = iter_wiki('/Users/viral.parikh/Desktop/External_Datasets/kaggle/20newsgroup/simplewiki-20140623-pages-articles.xml.bz2')\n",
    "for title, tokens in itertools.islice(iter_wiki('/Users/viral.parikh/Desktop/External_Datasets/kaggle/20newsgroup/simplewiki-20140623-pages-articles.xml.bz2'), 8):\n",
    "    print title, tokens[:10]  # print the article title and its first ten tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object iter_wiki at 0x109ccd8c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word = {0: u'word', 2: u'profit', 300: u'another_word'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: u'word', 2: u'profit', 300: u'another_word'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_stream = (tokens for _, tokens in iter_wiki('/Users/viral.parikh/Desktop/External_Datasets/kaggle/20newsgroup/simplewiki-20140623-pages-articles.xml.bz2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x109ccdf50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO:gensim.corpora.dictionary:adding document #10000 to Dictionary(148230 unique tokens: [u'fawn', u'\\u03c9\\u0431\\u0440\\u0430\\u0434\\u043e\\u0432\\u0430\\u043d\\u043d\\u0430\\u0467', u'refreshable', u'yollar\\u0131', u'idaira']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #20000 to Dictionary(225175 unique tokens: [u'biennials', u'sowela', u'mdbg', u'clottes', u'idaira']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #30000 to Dictionary(286134 unique tokens: [u'biennials', u'sowela', u'mdbg', u'clottes', u'klatki']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #40000 to Dictionary(375996 unique tokens: [u'biennials', u'sowela', u'mdbg', u'biysk', u'sermersheim']...)\n",
      "INFO:gensim.corpora.dictionary:built Dictionary(409123 unique tokens: [u'biennials', u'sowela', u'mdbg', u'biysk', u'sermersheim']...) from 48321 documents (total 10370158 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 36s, sys: 1.42 s, total: 3min 37s\n",
      "Wall time: 3min 37s\n",
      "Dictionary(409123 unique tokens: [u'biennials', u'sowela', u'mdbg', u'biysk', u'sermersheim']...)\n"
     ]
    }
   ],
   "source": [
    "%time id2word_wiki = gensim.corpora.Dictionary(doc_stream)\n",
    "print(id2word_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ignore words that appear in less than 20 documents or more than 10% documents\n",
    "id2word_wiki.filter_extremes(no_below=20, no_above=0.1)\n",
    "print(id2word_wiki)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
