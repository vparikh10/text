{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling w NMF (Used Scikit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Available - http://mlg.ucd.ie/datasets/bbc.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is a key tool for the discovery of latent semantic structure within document collections, where probabilistic models such as Latent Dirichlet allocation (LDA) are widely-used. \n",
    "\n",
    "However, a highly-effective alternative is to use Non-negative Matrix Factorization (NMF). This notebook provides a simple example of using the NMF implementation from scikit-learn to find topics in a small collection of news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, import everything required from scikit-learn and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, os.path, codecs\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For Mac OS\n",
    "dir_data=r'/Users/viral.parikh/Desktop/External_Datasets/kaggle/bbc/bbc_small'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a corpus by reading plain text files into a list of strings. The sample collection of 1162 BBC news articles used in this example can be downloaded from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 1162 corpus of documents\n"
     ]
    }
   ],
   "source": [
    "file_paths = [os.path.join(dir_data, fname) for fname in os.listdir(dir_data) if fname.endswith(\".txt\") ]\n",
    "documents = [codecs.open(file_path, 'r', encoding=\"utf8\", errors='ignore').read() for file_path in file_paths ]\n",
    "print \"Read %d corpus of documents\" % len(documents)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# file_paths\n",
    "#documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply tokenization and vectorization to build a document-term matrix A for the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1162, 6045)\n"
     ]
    }
   ],
   "source": [
    "#can specify max_features,tokenzier, ngram_range\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, lowercase=True, strip_accents=\"unicode\", use_idf=True, norm=\"l2\", min_df = 5) \n",
    "\n",
    "A = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the list of all terms for later use, whose indices correspond to the columns of the document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6045\n"
     ]
    }
   ],
   "source": [
    "num_terms = len(tfidf_vectorizer.vocabulary_)\n",
    "print num_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms = [\"\"] * num_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created document-term matrix of size 1162 x 6045\n"
     ]
    }
   ],
   "source": [
    "for term in tfidf_vectorizer.vocabulary_.keys():\n",
    "    terms[tfidf_vectorizer.vocabulary_[term]] = term\n",
    "    \n",
    "print \"Created document-term matrix of size %d x %d\" % (A.shape[0],A.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply NMF with SVD-based initialization to the document-term matrix A generate 4 topics, and get the factors W and H from the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = decomposition.NMF(init=\"nndsvd\", n_components=6, max_iter=200)\n",
    "W = model.fit_transform(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1162, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.        ,  0.02920229,  0.15534348,  0.00041028, -0.        ,\n",
       "         0.04660398],\n",
       "       [ 0.00872395, -0.        ,  0.15982083,  0.00684455,  0.01738618,\n",
       "         0.00053172],\n",
       "       [-0.        , -0.        ,  0.0419963 , -0.        ,  0.09228407,\n",
       "        -0.        ],\n",
       "       ..., \n",
       "       [ 0.01349794,  0.12488964,  0.02907738,  0.00093087,  0.00422005,\n",
       "         0.0348679 ],\n",
       "       [-0.        ,  0.20169135, -0.        , -0.        , -0.        ,\n",
       "         0.00902608],\n",
       "       [ 0.01178719,  0.03192502,  0.00473044,  0.00313964,  0.03883936,\n",
       "         0.20023313]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print W.shape\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated factor W of size (1162, 6) and factor H of size (6, 6045)\n"
     ]
    }
   ],
   "source": [
    "H = model.components_\n",
    "print \"Generated factor W of size %s and factor H of size %s\" % (str(W.shape), str(H.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 6045)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  2.64024097e-02,   1.60538034e-02,   1.52582236e-02, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   5.71339566e-04],\n",
       "       [  7.71661041e-02,   4.92047458e-02,   4.23580669e-02, ...,\n",
       "          3.76352143e-05,   6.81016309e-03,   0.00000000e+00],\n",
       "       [  1.04295564e-01,   7.52195989e-02,   2.21729951e-02, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   1.08614941e-02],\n",
       "       [  5.49892816e-03,   4.10755009e-02,   4.94313569e-03, ...,\n",
       "          1.52466446e-02,   2.19416981e-03,   6.28886334e-03],\n",
       "       [  7.64601474e-02,   1.41698988e-02,   2.43940915e-02, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  2.82531445e-02,   2.22283426e-02,   1.51616239e-02, ...,\n",
       "          0.00000000e+00,   7.34599675e-03,   2.79332146e-03]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print H.shape\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the top 10 ranked terms for each topic, by sorting the values in the rows of the H factor (i.e. the weights for each of the 6045 terms relative to the 4 topics found by NMF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: mr, blair, labour, brown, election, party, prime, minister, chancellor, howard\n",
      "Topic 1: mobile, phone, music, digital, people, technology, broadband, video, games, phones\n",
      "Topic 2: growth, economy, year, sales, bank, said, economic, prices, 2004, market\n",
      "Topic 3: chelsea, game, club, league, arsenal, united, cup, liverpool, players, mourinho\n",
      "Topic 4: said, government, law, lord, mr, police, court, lords, rights, home\n",
      "Topic 5: microsoft, virus, software, search, users, spyware, security, mail, program, windows\n"
     ]
    }
   ],
   "source": [
    "for topic_index in range(H.shape[0]):\n",
    "    top_indices = np.argsort(H[topic_index,:])[::-1][0:10]\n",
    "    term_ranking = [terms[i] for i in top_indices]\n",
    "    print \"Topic %d: %s\" % (topic_index, \", \".join(term_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reference - \n",
    "\n",
    "1. http://derekgreene.com/nmf-topic/\n",
    "2. http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "3. http://derekgreene.com/slides/nmf_insight_workshop.pdf\n",
    "4. http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html#example-applications-topics-extraction-with-nmf-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
